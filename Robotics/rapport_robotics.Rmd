---
title: "Robotics"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_notebook: default
---

# \texttt{Robotics} dataset

Le jeu de données d'apprentissage de ce problème représente la cinématique du bras d'un robot. Les predicteurs ainsi la réponse données sont toutes de type numérique. Le but est de trouver la relation de la réponse \texttt{y} et les 8 prédicteurs \texttt{X1, X2, X3, X4, X5, X6, X7, X8}. Il s'agit bien d'un problème de régression.

```{r include=FALSE  }
#Preparation des donnees
setwd("D:/文档/SY19/TD/TD4/sy19_tp10/Robotics")

data <- read.table("robotics_train.txt", header = TRUE)
data.scale <- scale(data)

set.seed(69)
n <- nrow(data)
p <- ncol(data) - 1
percentage <- 2/3
set.seed(69)
ntrain <- as.integer(n * percentage)
ntest <- n - ntrain
id.train <- sample(n, ntrain)
data.train <- data[id.train, ]
data.train.scale <- scale(data[id.train, ])

data.test <- data[-id.train, ]

y.train <- data.train[, 9]
x.train <- data.train[, -9]
y.test <- data.test[, 9]
x.test <- data.test[, -9]

x.train.scale <- scale(data.train[, -9])
x.test.scale <- scale(data.test[, -9])
```

## Analyse exploratoire

Dans un premier temps, on essaie de regarder la plage de toutes les données.

```{r echo=FALSE, fig.align='center', out.height='80%', out.width='50%'}
boxplot(as.data.frame(data[, 1:8]))
```

On observe que la plage de données est très homogène, ce qui nous donne la possibilité d'explorer nos données sans faire un scaling.

Ensuite on vérifie la corrélation entre les variables.

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.height='60%', out.width='50%'}
if(!require("corrplot")) {
  install.packages("corrplot")
  library("corrplot")
}

mcor <- cor(data[,c(1:9)])
corrplot(mcor, type="upper",method = "number",order="hclust", tl.col="black", tl.srt=45)#Les variables ne sont pas fortement correle

```

On constate qu'il existe pas de corrélations significatives entre les variables, seule de faibles corrélations entre la réponse et les prédicteurs. Ce graphe exclut le besoin d'enlever certaines variables puisqu'elles sont peu corrélées. On confirme cette observation en faisant une ACP:

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.height='80%', out.width='50%'}
if(!require("FactoMineR")) {
  install.packages("FactoMineR")
  library("FactoMineR")
}

if(!require("factoextra")) {
  install.packages("factoextra")
  library("factoextra")
}

res.pca <- PCA(data, ncp=p, quali.sup = p+1, graph = FALSE)
#print(res.pca)
fviz_eig(res.pca, ncp = 8, addlabels = TRUE, ylim = c(0, 20))#On voit que la variance sont expliquee par toutes les variables

```

On constate avec ce graphe que la variance est expliquée par toutes les variables. Et on en déduit que toutes les méthodes concernant **Subset Selection** eront pas utiles vu que les variables devraient toutes être incluses.

## Sélection de modèle

On test sur le jeu de données \texttt{Robotics} les méthodes suivantes, que l'on a vues en cours:\

1. Modèle linéaire

```{r eval=FALSE, include=FALSE}
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}

err.lin.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.train <- data[-index.outer.cv,]
  data.test <- data[index.outer.cv,]
  
  # linear regression
  model.fit <- lm(y~., data=data.train)
  summary(model.fit)
  pref <- predict(model.fit, newdata=data.test)
  err.lin.mse[i] = mean((data.test$y -pref)^2)
  
}
boxplot(err.lin.mse)
mean(err.lin.mse) #0.04185472


```

2.  La méthode des \textbf{KNN}

```{r eval=FALSE, include=FALSE}
if(!require("FNN")) {
  install.packages("FNN")
  library("FNN")
}

set.seed(69)
K<-10
folds=sample(1:K,n,replace=TRUE)
#CV sans scaling
CV.noscale<-rep(0,10)
for(i in (1:10)){
  for(local_k in (1:K)){
    knn.noscale<-knn.reg(train = data[folds!=local_k, -9], test = data[folds==local_k,-9], 
                         y = data[folds!=local_k, 9], k = i)
    #pred<-predict(reg,newdata=data[folds==k,])
    CV.noscale[i]<-CV.noscale[i]+ sum((data[folds==local_k, 9]- knn.noscale$pred)^2)
  }
  CV.noscale[i]<-CV.noscale[i]/n
}
boxplot( CV.noscale)
#min(CV.noscale)

#CV avec scaling
set.seed(69)
CV.scale<-rep(0,10)
for(i in (1:10)){
  for(local_k in (1:K)){
    knn.scale<-knn.reg(train = data.scale[folds!=local_k, -9], test = data.scale[folds==local_k,-9], 
                       y = data.scale[folds!=local_k, 9], k = i)
    #pred<-predict(reg,newdata=data[folds==k,])
    CV.scale[i]<-CV.scale[i]+ sum((data.scale[folds==local_k, 9]- knn.scale$pred)^2)
  }
  CV.scale[i]<-CV.scale[i]/n
}
plot(1:K, CV.scale, type = 'b', col = 'red')
which.min(CV.scale)
CV.scale[which.min(CV.scale)]

data.frame("scale" = c("NON","OUI"), "k" = c(which.min(CV.noscale), which.min(CV.scale)), 
           "MSE" = c(CV.noscale[which.min(CV.noscale)], CV.scale[which.min(CV.scale)]))
cat("On voit que KNN fonctionne mieux que le modele lineaire")#0.01608

```

3.  Les méthode de régularization \textbf{Ridge Regression, Lasso Regression, Elastic Regression}

```{r eval=FALSE, include=FALSE}
#Librarie
if(!require("glmnet")) {
  install.packages("glmnet")
  library("glmnet")
}

par(mfrow = c(1, 1))
#Donnees
x<-model.matrix(y~.,data)
y<-data$y

#Ridge regression----------------------------
#cv.out.ridge <- cv.glmnet(x.train.regu, y.train.regu, alpha = 0, type.measure = "mse")
#plot(cv.out.ridge)
#fit.ridge <- glmnet(x.train.regu, y.train.regu, lambda = cv.out.ridge$lambda.min, alpha = 0)
#ridge.predict <- predict(fit.ridge, s = cv.out.ridge$lambda.min, newx = x.test.regu)
#mse.ridge <- mean((ridge.predict - y.test.regu) ^ 2)#0.04147

#CV
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}
err.rid.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  # re-sampling (inner cross validation)
  alpha <- 0
  
  # validation model 
  x.train.regu <- data.matrix(data.inner[, -ncol(data)])
  y.train.regu <- data.inner[, ncol(data)]
  x.test.regu <- data.matrix(data.validation[, -ncol(data)])
  y.test.regu <- data.validation[, ncol(data)]
  cv.model <- cv.glmnet(x.train.regu, y.train.regu, alpha = alpha)
  model.fit <- glmnet(x.train.regu, y.train.regu, lambda = cv.model$lambda.min, alpha = alpha)
  err.rid.mse[i] <- mean((y.test.regu - predict(model.fit, newx=x.test.regu))^2)
}
boxplot(err.rid.mse)
#mean(err.rid.mse) #0.0419

#Lasso regression-----------------------------
#CV
err.las.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  # re-sampling (inner cross validation)
  alpha <- 1
  
  # validation model 
  x.train.regu <- data.inner[, -ncol(data)]
  y.train.regu <- data.inner[, ncol(data)]
  x.test.regu <- data.validation[, -ncol(data)]
  y.test.regu <- data.validation[, ncol(data)]
  cv.model <- cv.glmnet(data.matrix(x.train.regu), y.train.regu, alpha = alpha)
  model.fit <- glmnet(data.matrix(x.train.regu), y.train.regu, lambda = cv.model$lambda.min, alpha = alpha)
  err.las.mse[i] <- mean((y.test.regu - predict(model.fit, newx=data.matrix(x.test.regu)))^2)
}

#boxplot(err.las.mse)
#mean(err.las.mse) #0.04185563

#elastic-----------------------------
#CV
n.alpha <- 10
err.ela.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  # re-sampling (inner cross validation)
  data.inner <- data.inner[rs.data.inner[[i]], ]
  alphas <- seq(0, 1, length.out =  n.alpha)
  ela.inner.mse <- rep(0, n.alpha)
  best.lambda.alpha <- rep(0, n.alpha)
  for(k in 1:length(alphas)){
    for(j in 1:10){ 
      index.inner.cv <- idx.test.inner[[i]][[j]] 
      data.train <- data.inner[-index.inner.cv, ]
      data.test <- data.inner[index.inner.cv, ]
      
      alpha <- alphas[k]
      print(alpha)
      X.cv.train <- data.matrix(data.inner[, -ncol(data)])
      y.cv.train <- data.inner[, ncol(data)]
      cv.model <- cv.glmnet(X.cv.train, y.cv.train, alpha=alpha)
      ela.inner.mse[k] <- ela.inner.mse[k] + cv.model$cvm[which(cv.model$lambda == cv.model$lambda.min)]
    }
  }
  idx.best <- which(ela.inner.mse == min(ela.inner.mse))
  best.lambda <- best.lambda.alpha[idx.best]
  best.alpha <- alphas[idx.best]
  # validation model 
  x.train.regu <- data.matrix(data.inner[, -ncol(data)])
  y.train.regu <- data.inner[, ncol(data)]
  x.test.regu <- data.matrix(data.validation[, -ncol(data)])
  y.test.regu <- data.validation[, ncol(data)]
  cv.model <- cv.glmnet(x.train.regu, y.train.regu, alpha=best.alpha)
  model.fit <- glmnet(x.train.regu, y.train.regu, lambda = cv.model$lambda.min, alpha = best.alpha)
  err.ela.mse[i] <- mean((y.test.regu - predict(model.fit, newx=x.test.regu))^2)
}
#plot(x = alphas, y = ela.inner.mse/10, type='l', ylab='Cv(alpha) Error', xlab="alpha")
#boxplot(err.ela.mse)
#mean(err.ela.mse)

```

4.  La méthode des \textbf{Splines}
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
###########
# Splines #
###########

#PRESQUE FINI#

#prepararion des folds
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}

err.splinesnaturel.mse <- rep(0,10)
err.gam.mse <- rep(0,10)

p.sequences <- seq(1,10)

set.seed(69)

# ______________________________________________________ 
# ________________ GAM _________________________________ 
# ______________________________________________________ 
library(gam)
#err.gam.mse <- rep(0,10)
p.sequences <- seq(1,10)


err.gam.mse <- c(0.04574154, 0.04170184, 0.04625588, 0.04802093, 0.04674564,
                 0.04822308, 0.04900323, 0.04652718, 0.05011012, 0.04637874)
mean(err.gam.mse) #0.04673483


# ______________________________________________________ 
# ________________ splines _____________________________
# ______________________________________________________

# ________________ natural splines _____________________
err.splinesnaturel.mse <- rep(0,10)
p.sequences <- seq(1,10)

for (i in 1:10) {
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  data.inner <- data.inner[rs.data.inner[[i]], ]
  
  sp.pmin <- rep(0, length(p.sequences))
  for(p in 1:length(p.sequences)){
    for(j in 1:10){
      index.inner.cv <- idx.test.inner[[i]][[j]] 
      data.train.splinesnaturel <- data.inner[-index.inner.cv, ]
      data.test.splinesnaturel <- data.inner[index.inner.cv, ]
      model.spline <- lm(y ~ +ns(X1, p.sequences[p])+ns(X2,p.sequences[p])+ns(X3, p.sequences[p])
                         +ns(X4, p.sequences[p]), data=data.train.splinesnaturel) 
      model.pred <- predict(model.spline,newdata=data.test.splinesnaturel)
      sp.pmin[p] <-  sp.pmin[p] + mean((data.test.splinesnaturel$y -model.pred)^2)
    }
  }
  idx.pmin <- which(min(sp.pmin) == sp.pmin)
  best.pmin <- p.sequences[idx.pmin]
  
  data.train.splinesnaturel <- data.inner
  data.test.splinesnaturel <- data.validation
  
  naturalspline <- lm(y ~ +ns(X1, best.pmin)+ns(X2,best.pmin)+ns(X3, best.pmin)
                      +ns(X4, best.pmin), data=data.train.splinesnaturel)
  pred <- predict(naturalspline, newdata=data.test.splinesnaturel)
  err.splinesnaturel.mse[i] = mean((data.test.splinesnaturel$y -pred)^2)
}

mean(err.splinesnaturel.mse) #0.04714431
```

5.  \textbf{SVM}

```{r eval=FALSE, include=FALSE}
#Package
if(!require("kernlab")) {
  install.packages("kernlab")
  library("kernlab")
}
#CV
CC<-c(0.001,0.01,0.1,1,100)#,1000)
epsilon <- seq(0, 1, 0.1)
N<-length(CC)
M<-5 # nombre de r??p??titions de la validation crois??e


#SVM lineaire-----------------------------------
#CV
err.svmlin.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  
  y.test.svmLin <- data.validation[, ncol(data)]
  svmfit<-ksvm(y~.,data=data.inner,scaled=TRUE,type="eps-svr", kernel="vanilladot",C=0.01)
  yhat<-predict(svmfit,newdata=data.validation) 
  err.svmlin.mse[i] <- mean((y.test.svmLin - yhat)^2)
}
boxplot(err.svmlin.mse)
mean(err.svmlin.mse) # 0.0427

#SVM laplacien--------------------------------
#Cross validation pour la valeur de C
#CC<-c(0.001,0.01,0.1,1,10,100)
#N<-length(CC)
#M<-5 # nombre de r??p??titions de la validation crois??e
#CV
set.seed(69)
err.svmlap.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  data.inner <- data.inner[rs.data.inner[[i]], ]
  y.test.svmLap <- data.validation[, ncol(data)]
  svmfit<-ksvm(y~.,data=data.inner,type="eps-svr", kernel="laplacedot",C=100)
  yhat<-predict(svmfit,newdata=data.validation) 
  err.svmlap.mse[i] <- mean((y.test.svmLap - yhat)^2)
}
boxplot(err.svmlap.mse)
mean(err.svmlap.mse) # 0.007916768


#SVM Gaussian--------------------------------
#Cross validation pour la valeur de C
#CC<-c(0.001,0.01,0.1,1,10,100)
#N<-length(CC)
#M<-5 # nombre de r??p??titions de la validation crois??e
#CV
set.seed(69)
err.svmgau.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  data.inner <- data.inner[rs.data.inner[[i]], ]
  y.test.svmgau <- data.validation[, ncol(data)]
  svmfit<-ksvm(y~.,data=data.inner,type="eps-svr", kernel="rbfdot",C=100)
  yhat<-predict(svmfit,newdata=data.validation) 
  err.svmgau.mse[i] <- mean((y.test.svmgau - yhat)^2)
}
boxplot(err.svmgau.mse)
mean(err.svmgau.mse) # 0.006989074

```

7.  \textbf{L'Arbre de Décision}

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Package
if(!require("rpart")) {
  install.packages("rpart")
  library("rpart")
}

if(!require("rpart.plot")) {
  install.packages("rpart.plot")
  library("rpart.plot")
}

if(!require("randomForest")) {
  install.packages("randomForest")
  library("randomForest")
}


#Fit Decision Tree avec CV
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)                 
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}
err.tree.mse <- rep(0,10)

for (i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  
  data.train.tree <- data.inner
  data.test.tree <- data.validation
  
  model.tree <- rpart(y~., data=data.train.tree, method="anova", control = rpart.control(xval = 10, minbucket = 2))
  pred <- predict(model.tree, newdata=data.test.tree)
  err.tree.mse[i] = mean((data.test.tree$y -pred)^2)
}

fitDT<-rpart(y~.,data=data.train,method="anova")
rpart.plot(fitDT, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)# Arbre initial


#Prune
err.tree.mse.prune <- rep(0,10)

for (i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  
  data.train.tree <- data.inner
  data.test.tree <- data.validation
  
  model.tree <- rpart(y~., data=data.train.tree, method="anova", control = rpart.control(xval = 10, minbucket = 2))
  pred <- predict(model.tree, newdata=data.test.tree)
  err.tree.mse[i] = mean((data.test.tree$y -pred)^2)
  i.min<-which.min(model.tree$cptable[,4])
  cp.opt1<-model.tree$cptable[i.min,1]
  
  pruned_tree <- prune(model.tree, cp=cp.opt1)
  preDT.prune <- predict(pruned_tree, newdata = data.test.tree)
  err.tree.mse.prune[i] <- mean((data.test.tree[, 9] - preDT.prune)^2)#0.0472833
}
boxplot(err.tree.mse, err.tree.mse.prune)


#Bagging
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}

err.randomForest.mse <- rep(0,10)
err.gam.mse <- rep(0,10)

p.sequences <- seq(1,10)

set.seed(69)
#Modele avec validation croisee interne
for (i in 1:10) {
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  data.train.RF <- data.inner[-index.inner.cv, ]
  data.test.RF <- data.inner[index.inner.cv, ]
  
  fitRF<-randomForest(y~.,data=data.train.RF,mtry = p)
  preRF <- predict(fitRF, newdata = data.test.RF)
  err.randomForest.mse[i] = mean((data.test.RF[, 9] - preRF)^2)
}

mean(err.randomForest.mse)#0.02532425

```

Dans les différents modèles que l'on a utilisé, on a calculé l'erreur quadratique en appliquant la méthode \textbf{Cross validation} avec 10-folds. Dans certains cas, on l'a également utilisé pour trouver l'hyper-paramètre.

Pour commencer on a fait une linéaire régression. En regardant le \texttt{Summary}, on a observé que toutes les variables étaient significatives, c'est ce à quoi on s'attend puisque elles sont peu corrélées. Néanmoins, la \texttt{Adjusted R-squared} est inférieure à 0.5, ce qui rélève qu'elles n'expliquent pas beaucoup la volatilité de la variable \texttt{y} à prédire. Le modèle linéaire donne une erreur en moyenne 0.04185.

On a ensuite effectué une régression en utilisant \textbf{KNN}. En utilisant \textbf{Cross validation}, on a trouvé le meilleur nombre de voisin 7 pour les données avec/sans scaling. On obtient une erreur en moyenne 0.016 pour les deux modèles, mais il n'existe pas un grand écart entre le modèle avec scaling et celui sans scaling.

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.height="50%", out.width="50%"}
if(!require("caret")) {
  install.packages("caret")
  library("caret")
}
set.seed(69)
knnFit.noscale <- train(x.train, y.train,
                        method = 'knn',
                        tuneLength = 20,
                        trControl = trainControl(method = 'cv'))
knnFit.scale <- train(x.train.scale, y.train,
                      method = 'knn',
                      tuneLength = 20,
                      trControl = trainControl(method = 'cv'))

trellis.par.set(caretTheme())
par(mfrow = c(1, 1)) 

plot(knnFit.noscale, type = 'b', col = 'blue', xlab = "K(Avec scalling)")
plot(knnFit.scale, type = 'b', col = 'red', xlab = "K(Sans Scaling)")


```

On a ensuite appliqué les méthodes de régularisation Ridge, Lasso et ElasticNet. La régression Ridge ne permet pas de séléctionner les prédicteurs et elle les inclut tous. La régression Lasso, quant à elle, permet de réduire le coefficient de certains prédicteurs à 0, elle est donc une méthode de sélection de variables. La méthode ElasticNet est comprise entre les deux dernière. Elle permet d'effectuer une sélection de modèle et réduire le coefficient des variables corrélées. On a estimé le paramètre de cette méthode en le faisant varier entre (0, 1).

Nous avons ensuite estimé les degrés de liberté par validation croisée imbriquée à l'aide de splines naturelles et de lissage en régression linéaire. Nous appliquons également un modèle GAM, qui suppose que l'influence de chaque variable d'entrée est additive : nous remplaçons ainsi le problème d'estimation de fonctions p-dimensionnelles par le problème d'estimation simultanée de p fonctions 1-dimensionnelles.

On a appliqué de différentes méthodes concernant l'arbre des décision. Dans un premier temps, on a appliqué l'arbre de décicion classique. On pourrait savoir l'importance de chaque prédicteur en affichant l'arbre complet. Pour que les résultats soient plus précises, on a ainsi utilisé \textbf{Prune} et \textbf{Bagging}. L'arbre de décision reste inchangé avec \textbf{Prune}

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.height="50%", out.width="50%", fig.cap="L'arbre dans les deux cas", fig.align='center'}
if(!require("rpart")) {
  install.packages("rpart")
  library("rpart")
}

if(!require("rpart.plot")) {
  install.packages("rpart.plot")
  library("rpart.plot")
}

if(!require("randomForest")) {
  install.packages("randomForest")
  library("randomForest")
}

#fitDT<-rpart(y~.,data=data.train,method="anova")
#rpart.plot(fitDT, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)# Arbre initial

#model.tree <- rpart(y~., data=data.train, method="anova", control = rpart.control(xval = 10, minbucket = 2))
#i.min<-which.min(model.tree$cptable[,4])
#cp.opt1<-model.tree$cptable[i.min,1]

#pruned_tree <- prune(model.tree, cp=cp.opt1)
#rpart.plot(pruned_tree, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)# Arbre initial
knitr::include_graphics("./tree.png")  


```

Enfin, on a appliqué le modèle SVM en utilisant de différent noyaux. Les noyaux le plus performants qu'on a observé sont le noyau laplacien et le noyeau gaussien. Pour estimer la valeur de l'hyper-paramètre C, on a utilisé \textbf{Cross validation}

```{r echo=FALSE, fig.align='default', fig.cap="Le choix de l'hyper-paramètre pour Linear Kernel, Laplace Kernel, Gaussian Kernel", fig.show='hold', out.height="30%", out.width="30%", message=FALSE, warning=FALSE}
knitr::include_graphics(c("./cv.svmLin.bmp","./cv.svmLap.png", "./cv.svmGau.bmp"))  
```
  
## Modèle retenu
```{r echo=FALSE, fig.cap="MSE pour Modèle linéaire, KNN, Lasso, Ridge, ElasticNet, Natural Spline, GAM Arbre de décision, Arbre de décicion élagué, Random Forest, SVM Linear Kernel, SVM Laplace Kernel, SVM Gaussian Kernel", warning=FALSE, message=FALSE}

#finalData <- as.data.frame(cbind(err.lin.mse, CV.noscale, err.las.mse, err.rid.mse, err.ela.mse, err.splinesnaturel.mse,err.gam.mse, err.tree.mse, err.tree.mse.prune, err.randomForest.mse, err.svmlin.mse, err.svmlap.mse, err.svmgau.mse))

#colnames(finalData) <- c("Linear model", "KNN", "Lasso", "Ridge", "ElasticNet", "NaturalSpline", "GAM", "Decision Tree", "Decision Tree with pruning", "RandomForest", "SVM Linear", "SVM Laplace", "SVM Gaussian")

#boxplot(finalData, cex.axis=0.4, ylab = "MSE par Cross Validation")
knitr::include_graphics("./boxplot_final.png")  

```

Nous avons donc obtenu comme modèle le plus performant : modèle \textbf{SVM} avec noyau gaussien, avec C=100. C'est ce modèle que l'on va utiliser pour prédire les cinématiques du bra d'un robot.

## **Données Communities**
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(width = 60)
matrix(runif(100), ncol = 20)
```

```{r, echo=FALSE}
#setwd("~/Travail/UTC/GI05/SY19/TD/td10/sy19_tp10")
setwd("D:/文档/SY19/TD/TD4/sy19_tp10")
communities <- read.csv('data/communities_train.csv')
#enlever les données non prédictives (données par le communities.names)
communities <- subset(communities, select = -c(county, community, communityname, fold))

#supprimer les colonnes qui ont un taux de NA supérieur à 50%
na_count <-sapply(communities, function(y) sum(length(which(is.na(y)))))
communities <- subset(communities, select = -c(which(na_count >= 500)))
#remplacer le NA restant dans OtherPerCap par la moyenne de cette colonne : 0.28 d'après communities.names
communities$OtherPerCap[is.na(communities$OtherPerCap)] <- 0.28

communities.X <- subset(communities, select=-c(ViolentCrimesPerPop))
communities.y <- communities$ViolentCrimesPerPop
#calculer la corrélation sans la variable nominale state
cor.communities.X <- round(cor(subset(communities.X, select=-c(state))), 2)
#les variables sont assez corrélées entre elles


#split train/test
n<-nrow(communities)
train<-sample(1:n,round(2*n/3))
communities.train<- communities[train,]
communities.test<-communities[-train,]
communities.X.train<-communities.X[train,]
communities.X.test<- communities.X[-train,]
communities.y.train<-communities.y[train]
communities.y.test<-communities.y[-train]
```


### Données manquantes

Le jeu de données *Communities* est composé de 128 variables explicatives et d'une donnée à prédire : *ViolentCrimesPerPop*. Ce jeu de données, en plus d'être très gros et complexe, possède beaucoup de valeurs manquantes, représentées par des NA. Il faut donc tout d'abord gérer ces données avant de les fournir aux modèles à apprendre.

Plusieurs solutions sont envisageables pour gérer les données manquantes. Dans notre cas, nous avons vérifié le taux de NA présents pour chaque variables explicatives. Si ce taux est supérieur à 50%, nous considérons que cette variable n'est pas assez "complète" pour pouvoir vraiment expliquer la variable à prédire. De ce fait, nous la supprimons carrément du jeu de données. Cette méthode nous a permis de supprimer 26 variables.

Dans le cas où le taux de NA dans une colonne est inférieur à 50%, nous remplaçons ces valeurs manquantes par la moyenne de cette variable, disponible dans le fichier *communities.names*. Cette solution s'appelle l'imputation de valeurs manquantes, et elle crée du biais dans le jeu de données. Ainsi, si nous abusons de cette méthode, l'apprentissage pourrait être totalement erroné.\
Dans notre cas, seule une variable possédait un taux de NA inférieur à 50%, et de plus, elle ne possédait en fait qu'une seule valeur manquante. Ainsi, l'imputation de cette valeur manquante n'ajoute qu'un biais très faible et négligeable, ce qui est un très bon compromis pour garder cette variable.

Le jeu de données ainsi obtenu ne contient plus que 102 variables sans aucune valeur manquante.

### Analyse rapide des données

Avant de se lancer dans l'apprentissage des modèles, nous nous sommes intéressés à la corrélation des variables entre elles. Nous nous sommes aperçus que les variables sont pour la plupart correlées mais peu ont coefficient de corrélation supérieur à 50/60%. Il est donc probable que certaines variables ne soient pas toutes utiles à l'apprentissage des modèles. Ceci est visible également lorsque nous faisons une ACP : 21 composantes suffisent pour avoir 90% de variance expliquée.

```{r, echo=FALSE, fig.show='hold', out.width='60%', fig.align="center"}
pca<-prcomp(subset(communities.X, select=-c(state)))
lambda<-pca$sdev^2
plot(cumsum(lambda)/sum(lambda),type="l",xlab="q", ylab="proportion of explained variance")
```

Nous avons donc décidé de privilégier des modèles simples, avec peu de variables explicatives, même si nous avons également testé des modèles plus complexes.

### Modèles testés

Les modèles testés sont les suivants : Régression linéaire, régression ridge et lasso, Arbre de décision, Random Forest, et SVM.

#### Régression linéaire avec sélection de variables

Nous avons premièrement testé un modèle linéaire avec toutes les variables du jeu de données. Les résultats obtenus étaient très élevés comparés aux autres (10 à 30 fois plus élevés). Nous avons donc testé un modèle linéaire en choisissant dix variables explicatives, sélectionnées grâce à la Backward subset selection. Nous avons ainsi obtenus les 10 variables avec les coefficients les plus significatifs, et nous les avons injecté dans notre modèle linéaire : *racepctblack, pctUrban, MalePctDivorce, PctKids2Par, PctPersDenseHous, PctHousOccup, RentLowQ, MedRent, MedOwnCostPctIncNoMtg,* et *NumStreet.*

Les résultats ainsi obtenus sont bien plus satisfaisants malgré la simplicité du modèle. Le modèle a donc l'air de bien s'y adapter et d'être robuste. Le MSE tourne autour de 0,019 et 0,022.

```{r, echo=FALSE, fig.show='hold', out.width='60%', fig.align="center"}
fit.lm <- lm(communities.y.train~racepctblack+pctUrban+MalePctDivorce+PctKids2Par+PctPersDenseHous+PctHousOccup+RentLowQ+MedRent+MedOwnCostPctIncNoMtg+NumStreet, data=communities.X.train)

pred_lm <-  predict(fit.lm, newdata=communities.X.test)
err_lm <- mean((communities.y.test-pred_lm)^2)
plot(communities.y.train, fitted(fit.lm))
abline(0,1)
```

A noter que nous avons également testé une GAM avec ces mêmes variables. Le résultat obtenu est plus ou moins le même lorsque nous n'utilisons pas de splines. L'influence de ces variables est donc potentiellement additive.

#### Régression ridge et lasso

Toujours dans l'optique de réduire le nombre de variables, nous avons testé ces deux modèles.

```{r, echo=FALSE, fig.show='hold', out.width='50%'}
library(glmnet)
x<- model.matrix(ViolentCrimesPerPop~., communities)
y<-communities.y
n<-nrow(x)
ntrain=2*n/3
ntest=n-ntrain
train<-sample(1:ntrain)
xtrain<-x[train,]
ytrain<-y[train]
xtest<-x[-train,]
ytest<-y[-train]

cv.out.ridge<-cv.glmnet(xtrain,ytrain,alpha=0)
cv.out.lasso<-cv.glmnet(xtrain,ytrain,alpha=1)
plot(cv.out.ridge)
plot(cv.out.lasso)
```

Les résultats ensuite obtenus grâce aux deux modèles sont relativement similaires, avec un MSE tournant au voisinage de 0.0187 pour le ridge, et 0.0182 pour le lasso. En observant les coefficients beta de chaque modèle, nous observons que le nombre de coefficients mis à zéro dans le lasso est très élevé (environ 80 coefficients pour une centaine de variables). Ceci est vérifiable également dans le régression ridge, car ces mêmes coefficients ont des valeurs très faibles (souvent d'un ordre de grandeur des centimèmes ou des millièmes). Ainsi, cela prouve bien que toutes les variables n'ont pas une grande influence sur la variable de réponse, et que certaines peuvent être rejetées, comme nous l'avons fait dans notre modèle linéaire précédemment.\
Ces deux modèles de régression ridge et lasso sont robustes et donnent quasiment les mêmes résultats quelques soient les splits train/test donnés.

#### Arbre de décision

Nous avons ensuite testé une méthode plus couramment utilisée pour la classification: les arbres de décision. Cet arbre a été généré à partir de toutes les variables explicatives puis élagué selon la méthode vu dans le TP7.

```{r, echo=FALSE, fig.show='hold', out.width='60%', fig.align="center"}
library(rpart)
fit.tree <-rpart(communities.y.train~.,data=communities.X.train,method="anova")
library(rpart.plot)
#rpart.plot(fit.tree, box.palette="RdBu", shadow.col="gray",fallen.leaves=FALSE)

#printcp(fit.tree)
#plotcp(fit.tree)

i.min <- which.min(fit.tree$cptable[,4])
cp.opt1 <- fit.tree$cptable[i.min]

pruned_tree <- prune(fit.tree, cp= cp.opt1)
rpart.plot(pruned_tree, box.palette="RdBu", shadow.col = "gray")
predict <- predict(pruned_tree, newdata=communities.X.test)
err.pruned_tree <- mean((communities.y.test - predict)^2)
```

Nous observons que seules 5 variables explicatives sont utilisées dans cet arbre. Malgré la simplicité de cet arbre, le MSE de ce dernier est d'environ 0.028. Cette valeur n'est pas si mauvaise contrairement au modèle linéaire contenant toutes les variables, mais ce modèle reste moins performant que ceux testés plus haut.

#### Random Forest

Une option plus robuste que l'arbre de décision sont les Random Forests. L'apprentissage de ce modèle nous a, sans surprise, donné de meilleurs résultats que ceux de l'arbre de décision, avec un MSE tournant autour de 0.019/0.020. Cette performance est similaire voire meilleure que les modèles que nous avons testé précédemment. Il est intéressant de noter que le graphe de l'importance des variables nous montre que les variables les plus importantes obtenues avec cette méthode sont parfois les mêmes que celles choisies par Backward Subset selection, vu plus haut. Enfin, grâce à la robustesse du modèle dû aux mélanges de plusieurs arbres de décision, cette méthode a l'air d'être un choix pertinent pour ce jeu de données. En effet, les Random Forests calculant différents arbres en retirant certaines variables prédictives puis en en faisant une moyenne globale, ils permettent de discriminer les variables qui n'ont que très peu d'influence sur la variable de réponse.

```{r, echo=FALSE, fig.show='hold', out.width='60%', fig.align="center"}
library(randomForest)
fit.RF<-randomForest(communities.y.train~.,data=communities.X.train,mtry=5,importance=TRUE)
pred.RF<- predict(fit.RF,newdata=communities.X.test,type="response")
err.RF<- mean((communities.y.test - pred.RF)^2)
varImpPlot(fit.RF, n.var=10)
```

#### SVM

Enfin, nous avons testé un SVM en utilisant la méthode vue en TD pour optimiser le métaparamètre C. Le MSE obtenu après avoir déterminé ce paramètre varie beaucoup en fonction du split train/test des données, allant de 0.020 jusqu'à 0.024 et plus. Cette méthode n'a donc pas l'air de bien s'adapter au jeu de données.

## Conclusion

Les différents modèles donnent des résultats plus ou moins bons. Malgré cette divergence de résultats, certains modèles permettent de confirmer l'importance (ou non) des différentes variables pour expliquer la variable *ViolentCrimesPerPop*. De plus, nous observons que la régression ridge et le lasso, le modèle linéaire à 10 variables explicatives, ainsi que le Random Forest sont plutôt performants et robustes de part leur simplicité. Nous avons donc choisi d'utiliser le Random Forest comme modèle final.

Pour conclure, ces modèles nous ont permis de mettre en avant l'importance de l'influence de certaines variables sur la variable de réponse, par exemple le taux de population afro-américaine, le pourcentage de foyer avec 2 parents possédant des enfants, ou le pourcentage d'habitations denses (avec plus d'une personnes par chambre). Il est intéressant de noter que d'un modèle à un autre, les variables les plus "importantes" sont différentes, mais certaines se retrouvent dans plusieurs modèles. Nous pouvons donc croire que ces dernières ont une forte influence sur la détermination de la variable à expliquer.