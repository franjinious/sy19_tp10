---
title: "Robotics"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_notebook: default
---

# \texttt{Robotics} dataset

Le jeu de données d'apprentissage de ce problème représente la cinématique du bras d'un robot. Les predicteurs ainsi la réponse données sont toutes de type numérique. Le but est de trouver la relation de la réponse \texttt{y} et les 8 prédicteurs \texttt{X1, X2, X3, X4, X5, X6, X7, X8}. Il s'agit bien d'un problème de régression.

```{r include=FALSE  }
#Preparation des donnees
setwd("D:/文档/SY19/TD/TD4/sy19_tp10/Robotics")

data <- read.table("robotics_train.txt", header = TRUE)
data.scale <- scale(data)

set.seed(69)
n <- nrow(data)
p <- ncol(data) - 1
percentage <- 2/3
set.seed(69)
ntrain <- as.integer(n * percentage)
ntest <- n - ntrain
id.train <- sample(n, ntrain)
data.train <- data[id.train, ]
data.train.scale <- scale(data[id.train, ])

data.test <- data[-id.train, ]

y.train <- data.train[, 9]
x.train <- data.train[, -9]
y.test <- data.test[, 9]
x.test <- data.test[, -9]

x.train.scale <- scale(data.train[, -9])
x.test.scale <- scale(data.test[, -9])
```

## Analyse exploratoire

Dans un premier temps, on essaie de regarder la plage de toutes les données.

```{r echo=FALSE, fig.align='center', out.height='80%', out.width='50%'}
boxplot(as.data.frame(data[, 1:8]))
```

On observe que la plage de données est très homogène, ce qui nous donne la possibilité d'explorer nos données sans faire un scaling.

Ensuite on vérifie la corrélation entre les variables.

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.height='60%', out.width='50%'}
if(!require("corrplot")) {
  install.packages("corrplot")
  library("corrplot")
}

mcor <- cor(data[,c(1:9)])
corrplot(mcor, type="upper",method = "number",order="hclust", tl.col="black", tl.srt=45)#Les variables ne sont pas fortement correle

```

On constate qu'il existe pas de corrélations significatives entre les variables, seule de faibles corrélations entre la réponse et les prédicteurs. Ce graphe exclut le besoin d'enlever certaines variables puisqu'elles sont peu corrélées. On confirme cette observation en faisant une ACP:

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.height='80%', out.width='50%'}
if(!require("FactoMineR")) {
  install.packages("FactoMineR")
  library("FactoMineR")
}

if(!require("factoextra")) {
  install.packages("factoextra")
  library("factoextra")
}

res.pca <- PCA(data, ncp=p, quali.sup = p+1, graph = FALSE)
#print(res.pca)
fviz_eig(res.pca, ncp = 8, addlabels = TRUE, ylim = c(0, 20))#On voit que la variance sont expliquee par toutes les variables

```

On constate avec ce graphe que la variance est expliquée par toutes les variables. Et on en déduit que toutes les méthodes concernant **Subset Selection** eront pas utiles vu que les variables devraient toutes être incluses.

## Sélection de modèle

On test sur le jeu de données \texttt{Robotics} les méthodes suivantes, que l'on a vues en cours:\

1. Modèle linéaire

```{r eval=FALSE, include=FALSE}
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}

err.lin.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.train <- data[-index.outer.cv,]
  data.test <- data[index.outer.cv,]
  
  # linear regression
  model.fit <- lm(y~., data=data.train)
  summary(model.fit)
  pref <- predict(model.fit, newdata=data.test)
  err.lin.mse[i] = mean((data.test$y -pref)^2)
  
}
boxplot(err.lin.mse)
mean(err.lin.mse) #0.04185472


```

2.  La méthode des \textbf{KNN}

```{r eval=FALSE, include=FALSE}
if(!require("FNN")) {
  install.packages("FNN")
  library("FNN")
}

set.seed(69)
K<-10
folds=sample(1:K,n,replace=TRUE)
#CV sans scaling
CV.noscale<-rep(0,10)
for(i in (1:10)){
  for(local_k in (1:K)){
    knn.noscale<-knn.reg(train = data[folds!=local_k, -9], test = data[folds==local_k,-9], 
                         y = data[folds!=local_k, 9], k = i)
    #pred<-predict(reg,newdata=data[folds==k,])
    CV.noscale[i]<-CV.noscale[i]+ sum((data[folds==local_k, 9]- knn.noscale$pred)^2)
  }
  CV.noscale[i]<-CV.noscale[i]/n
}
boxplot( CV.noscale)
#min(CV.noscale)

#CV avec scaling
set.seed(69)
CV.scale<-rep(0,10)
for(i in (1:10)){
  for(local_k in (1:K)){
    knn.scale<-knn.reg(train = data.scale[folds!=local_k, -9], test = data.scale[folds==local_k,-9], 
                       y = data.scale[folds!=local_k, 9], k = i)
    #pred<-predict(reg,newdata=data[folds==k,])
    CV.scale[i]<-CV.scale[i]+ sum((data.scale[folds==local_k, 9]- knn.scale$pred)^2)
  }
  CV.scale[i]<-CV.scale[i]/n
}
plot(1:K, CV.scale, type = 'b', col = 'red')
which.min(CV.scale)
CV.scale[which.min(CV.scale)]

data.frame("scale" = c("NON","OUI"), "k" = c(which.min(CV.noscale), which.min(CV.scale)), 
           "MSE" = c(CV.noscale[which.min(CV.noscale)], CV.scale[which.min(CV.scale)]))
cat("On voit que KNN fonctionne mieux que le modele lineaire")#0.01608

```

3.  Les méthode de régularization \textbf{Ridge Regression, Lasso Regression, Elastic Regression}

```{r eval=FALSE, include=FALSE}
#Librarie
if(!require("glmnet")) {
  install.packages("glmnet")
  library("glmnet")
}

par(mfrow = c(1, 1))
#Donnees
x<-model.matrix(y~.,data)
y<-data$y

#Ridge regression----------------------------
#cv.out.ridge <- cv.glmnet(x.train.regu, y.train.regu, alpha = 0, type.measure = "mse")
#plot(cv.out.ridge)
#fit.ridge <- glmnet(x.train.regu, y.train.regu, lambda = cv.out.ridge$lambda.min, alpha = 0)
#ridge.predict <- predict(fit.ridge, s = cv.out.ridge$lambda.min, newx = x.test.regu)
#mse.ridge <- mean((ridge.predict - y.test.regu) ^ 2)#0.04147

#CV
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}
err.rid.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  # re-sampling (inner cross validation)
  alpha <- 0
  
  # validation model 
  x.train.regu <- data.matrix(data.inner[, -ncol(data)])
  y.train.regu <- data.inner[, ncol(data)]
  x.test.regu <- data.matrix(data.validation[, -ncol(data)])
  y.test.regu <- data.validation[, ncol(data)]
  cv.model <- cv.glmnet(x.train.regu, y.train.regu, alpha = alpha)
  model.fit <- glmnet(x.train.regu, y.train.regu, lambda = cv.model$lambda.min, alpha = alpha)
  err.rid.mse[i] <- mean((y.test.regu - predict(model.fit, newx=x.test.regu))^2)
}
boxplot(err.rid.mse)
#mean(err.rid.mse) #0.0419

#Lasso regression-----------------------------
#CV
err.las.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  # re-sampling (inner cross validation)
  alpha <- 1
  
  # validation model 
  x.train.regu <- data.inner[, -ncol(data)]
  y.train.regu <- data.inner[, ncol(data)]
  x.test.regu <- data.validation[, -ncol(data)]
  y.test.regu <- data.validation[, ncol(data)]
  cv.model <- cv.glmnet(data.matrix(x.train.regu), y.train.regu, alpha = alpha)
  model.fit <- glmnet(data.matrix(x.train.regu), y.train.regu, lambda = cv.model$lambda.min, alpha = alpha)
  err.las.mse[i] <- mean((y.test.regu - predict(model.fit, newx=data.matrix(x.test.regu)))^2)
}

#boxplot(err.las.mse)
#mean(err.las.mse) #0.04185563

#elastic-----------------------------
#CV
n.alpha <- 10
err.ela.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  # re-sampling (inner cross validation)
  data.inner <- data.inner[rs.data.inner[[i]], ]
  alphas <- seq(0, 1, length.out =  n.alpha)
  ela.inner.mse <- rep(0, n.alpha)
  best.lambda.alpha <- rep(0, n.alpha)
  for(k in 1:length(alphas)){
    for(j in 1:10){ 
      index.inner.cv <- idx.test.inner[[i]][[j]] 
      data.train <- data.inner[-index.inner.cv, ]
      data.test <- data.inner[index.inner.cv, ]
      
      alpha <- alphas[k]
      print(alpha)
      X.cv.train <- data.matrix(data.inner[, -ncol(data)])
      y.cv.train <- data.inner[, ncol(data)]
      cv.model <- cv.glmnet(X.cv.train, y.cv.train, alpha=alpha)
      ela.inner.mse[k] <- ela.inner.mse[k] + cv.model$cvm[which(cv.model$lambda == cv.model$lambda.min)]
    }
  }
  idx.best <- which(ela.inner.mse == min(ela.inner.mse))
  best.lambda <- best.lambda.alpha[idx.best]
  best.alpha <- alphas[idx.best]
  # validation model 
  x.train.regu <- data.matrix(data.inner[, -ncol(data)])
  y.train.regu <- data.inner[, ncol(data)]
  x.test.regu <- data.matrix(data.validation[, -ncol(data)])
  y.test.regu <- data.validation[, ncol(data)]
  cv.model <- cv.glmnet(x.train.regu, y.train.regu, alpha=best.alpha)
  model.fit <- glmnet(x.train.regu, y.train.regu, lambda = cv.model$lambda.min, alpha = best.alpha)
  err.ela.mse[i] <- mean((y.test.regu - predict(model.fit, newx=x.test.regu))^2)
}
#plot(x = alphas, y = ela.inner.mse/10, type='l', ylab='Cv(alpha) Error', xlab="alpha")
#boxplot(err.ela.mse)
#mean(err.ela.mse)

```

4.  La méthode des \textbf{Splines}
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
###########
# Splines #
###########

#PRESQUE FINI#

#prepararion des folds
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}

err.splinesnaturel.mse <- rep(0,10)
err.gam.mse <- rep(0,10)

p.sequences <- seq(1,10)

set.seed(69)

# ______________________________________________________ 
# ________________ GAM _________________________________ 
# ______________________________________________________ 
library(gam)
#err.gam.mse <- rep(0,10)
p.sequences <- seq(1,10)


err.gam.mse <- c(0.04574154, 0.04170184, 0.04625588, 0.04802093, 0.04674564,
                 0.04822308, 0.04900323, 0.04652718, 0.05011012, 0.04637874)
mean(err.gam.mse) #0.04673483


# ______________________________________________________ 
# ________________ splines _____________________________
# ______________________________________________________

# ________________ natural splines _____________________
err.splinesnaturel.mse <- rep(0,10)
p.sequences <- seq(1,10)

for (i in 1:10) {
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  data.inner <- data.inner[rs.data.inner[[i]], ]
  
  sp.pmin <- rep(0, length(p.sequences))
  for(p in 1:length(p.sequences)){
    for(j in 1:10){
      index.inner.cv <- idx.test.inner[[i]][[j]] 
      data.train.splinesnaturel <- data.inner[-index.inner.cv, ]
      data.test.splinesnaturel <- data.inner[index.inner.cv, ]
      model.spline <- lm(y ~ +ns(X1, p.sequences[p])+ns(X2,p.sequences[p])+ns(X3, p.sequences[p])
                         +ns(X4, p.sequences[p]), data=data.train.splinesnaturel) 
      model.pred <- predict(model.spline,newdata=data.test.splinesnaturel)
      sp.pmin[p] <-  sp.pmin[p] + mean((data.test.splinesnaturel$y -model.pred)^2)
    }
  }
  idx.pmin <- which(min(sp.pmin) == sp.pmin)
  best.pmin <- p.sequences[idx.pmin]
  
  data.train.splinesnaturel <- data.inner
  data.test.splinesnaturel <- data.validation
  
  naturalspline <- lm(y ~ +ns(X1, best.pmin)+ns(X2,best.pmin)+ns(X3, best.pmin)
                      +ns(X4, best.pmin), data=data.train.splinesnaturel)
  pred <- predict(naturalspline, newdata=data.test.splinesnaturel)
  err.splinesnaturel.mse[i] = mean((data.test.splinesnaturel$y -pred)^2)
}

mean(err.splinesnaturel.mse) #0.04714431
```

5.  \textbf{SVM}

```{r eval=FALSE, include=FALSE}
#Package
if(!require("kernlab")) {
  install.packages("kernlab")
  library("kernlab")
}
#CV
CC<-c(0.001,0.01,0.1,1,100)#,1000)
epsilon <- seq(0, 1, 0.1)
N<-length(CC)
M<-5 # nombre de r??p??titions de la validation crois??e


#SVM lineaire-----------------------------------
#CV
err.svmlin.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  
  y.test.svmLin <- data.validation[, ncol(data)]
  svmfit<-ksvm(y~.,data=data.inner,scaled=TRUE,type="eps-svr", kernel="vanilladot",C=0.01)
  yhat<-predict(svmfit,newdata=data.validation) 
  err.svmlin.mse[i] <- mean((y.test.svmLin - yhat)^2)
}
boxplot(err.svmlin.mse)
mean(err.svmlin.mse) # 0.0427

#SVM laplacien--------------------------------
#Cross validation pour la valeur de C
#CC<-c(0.001,0.01,0.1,1,10,100)
#N<-length(CC)
#M<-5 # nombre de r??p??titions de la validation crois??e
#CV
set.seed(69)
err.svmlap.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  data.inner <- data.inner[rs.data.inner[[i]], ]
  y.test.svmLap <- data.validation[, ncol(data)]
  svmfit<-ksvm(y~.,data=data.inner,type="eps-svr", kernel="laplacedot",C=100)
  yhat<-predict(svmfit,newdata=data.validation) 
  err.svmlap.mse[i] <- mean((y.test.svmLap - yhat)^2)
}
boxplot(err.svmlap.mse)
mean(err.svmlap.mse) # 0.007916768


#SVM Gaussian--------------------------------
#Cross validation pour la valeur de C
#CC<-c(0.001,0.01,0.1,1,10,100)
#N<-length(CC)
#M<-5 # nombre de r??p??titions de la validation crois??e
#CV
set.seed(69)
err.svmgau.mse <-  rep(0, 10)
for(i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  data.inner <- data.inner[rs.data.inner[[i]], ]
  y.test.svmgau <- data.validation[, ncol(data)]
  svmfit<-ksvm(y~.,data=data.inner,type="eps-svr", kernel="rbfdot",C=100)
  yhat<-predict(svmfit,newdata=data.validation) 
  err.svmgau.mse[i] <- mean((y.test.svmgau - yhat)^2)
}
boxplot(err.svmgau.mse)
mean(err.svmgau.mse) # 0.006989074

```

7.  \textbf{L'Arbre de Décision}

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Package
if(!require("rpart")) {
  install.packages("rpart")
  library("rpart")
}

if(!require("rpart.plot")) {
  install.packages("rpart.plot")
  library("rpart.plot")
}

if(!require("randomForest")) {
  install.packages("randomForest")
  library("randomForest")
}


#Fit Decision Tree avec CV
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)                 
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}
err.tree.mse <- rep(0,10)

for (i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  
  data.train.tree <- data.inner
  data.test.tree <- data.validation
  
  model.tree <- rpart(y~., data=data.train.tree, method="anova", control = rpart.control(xval = 10, minbucket = 2))
  pred <- predict(model.tree, newdata=data.test.tree)
  err.tree.mse[i] = mean((data.test.tree$y -pred)^2)
}

fitDT<-rpart(y~.,data=data.train,method="anova")
rpart.plot(fitDT, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)# Arbre initial


#Prune
err.tree.mse.prune <- rep(0,10)

for (i in 1:10){
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  
  
  data.train.tree <- data.inner
  data.test.tree <- data.validation
  
  model.tree <- rpart(y~., data=data.train.tree, method="anova", control = rpart.control(xval = 10, minbucket = 2))
  pred <- predict(model.tree, newdata=data.test.tree)
  err.tree.mse[i] = mean((data.test.tree$y -pred)^2)
  i.min<-which.min(model.tree$cptable[,4])
  cp.opt1<-model.tree$cptable[i.min,1]
  
  pruned_tree <- prune(model.tree, cp=cp.opt1)
  preDT.prune <- predict(pruned_tree, newdata = data.test.tree)
  err.tree.mse.prune[i] <- mean((data.test.tree[, 9] - preDT.prune)^2)#0.0472833
}
boxplot(err.tree.mse, err.tree.mse.prune)


#Bagging
group.outer <- rep((1:10), (n/10)+1)[1:n]
idx.test.outer <- list()
idx.test.inner <- list()
rs.data.inner <- list()
for(i in 1:10){
  index.cv <- which(group.outer==i)
  idx.test.outer[[i]] <- index.cv
  n.inner <- n - length(index.cv)
  rs.data.inner[[i]] <- sample(n.inner)
  group.inner <- rep((1:10), (n.inner/10)+1)[1:n.inner]
  idx.test.inner[[i]] <- list()
  for(j in 1:10){
    index.inner.cv <- which(group.inner==j)
    idx.test.inner[[i]][[j]] <- index.inner.cv
  }
}

err.randomForest.mse <- rep(0,10)
err.gam.mse <- rep(0,10)

p.sequences <- seq(1,10)

set.seed(69)
#Modele avec validation croisee interne
for (i in 1:10) {
  index.outer.cv <- idx.test.outer[[i]]
  data.inner <- data[-index.outer.cv,]
  data.validation <- data[index.outer.cv,]
  data.train.RF <- data.inner[-index.inner.cv, ]
  data.test.RF <- data.inner[index.inner.cv, ]
  
  fitRF<-randomForest(y~.,data=data.train.RF,mtry = p)
  preRF <- predict(fitRF, newdata = data.test.RF)
  err.randomForest.mse[i] = mean((data.test.RF[, 9] - preRF)^2)
}

mean(err.randomForest.mse)#0.02532425

```

Dans les différents modèles que l'on a utilisé, on a calculé l'erreur quadratique en appliquant la méthode \textbf{Cross validation} avec 10-folds. Dans certains cas, on l'a également utilisé pour trouver l'hyper-paramètre.

Pour commencer on a fait une linéaire régression. En regardant le \texttt{Summary}, on a observé que toutes les variables étaient significatives, c'est ce à quoi on s'attend puisque elles sont peu corrélées. Néanmoins, la \texttt{Adjusted R-squared} est inférieure à 0.5, ce qui rélève qu'elles n'expliquent pas beaucoup la volatilité de la variable \texttt{y} à prédire. Le modèle linéaire donne une erreur en moyenne 0.04185.

On a ensuite effectué une régression en utilisant \textbf{KNN}. En utilisant \textbf{Cross validation}, on a trouvé le meilleur nombre de voisin 7 pour les données avec/sans scaling. On obtient une erreur en moyenne 0.016 pour les deux modèles, mais il n'existe pas un grand écart entre le modèle avec scaling et celui sans scaling.

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.height="50%", out.width="50%"}
if(!require("caret")) {
  install.packages("caret")
  library("caret")
}
set.seed(69)
knnFit.noscale <- train(x.train, y.train,
                        method = 'knn',
                        tuneLength = 20,
                        trControl = trainControl(method = 'cv'))
knnFit.scale <- train(x.train.scale, y.train,
                      method = 'knn',
                      tuneLength = 20,
                      trControl = trainControl(method = 'cv'))

trellis.par.set(caretTheme())
par(mfrow = c(1, 1)) 

plot(knnFit.noscale, type = 'b', col = 'blue', xlab = "K(Avec scalling)")
plot(knnFit.scale, type = 'b', col = 'red', xlab = "K(Sans Scaling)")


```

On a ensuite appliqué les méthodes de régularisation Ridge, Lasso et ElasticNet. La régression Ridge ne permet pas de séléctionner les prédicteurs et elle les inclut tous. La régression Lasso, quant à elle, permet de réduire le coefficient de certains prédicteurs à 0, elle est donc une méthode de sélection de variables. La méthode ElasticNet est comprise entre les deux dernière. Elle permet d'effectuer une sélection de modèle et réduire le coefficient des variables corrélées. On a estimé le paramètre de cette méthode en le faisant varier entre (0, 1).

Nous avons ensuite estimé les degrés de liberté par validation croisée imbriquée à l'aide de splines naturelles et de lissage en régression linéaire. Nous appliquons également un modèle GAM, qui suppose que l'influence de chaque variable d'entrée est additive : nous remplaçons ainsi le problème d'estimation de fonctions p-dimensionnelles par le problème d'estimation simultanée de p fonctions 1-dimensionnelles.

On a appliqué de différentes méthodes concernant l'arbre des décision. Dans un premier temps, on a appliqué l'arbre de décicion classique. On pourrait savoir l'importance de chaque prédicteur en affichant l'arbre complet. Pour que les résultats soient plus précises, on a ainsi utilisé \textbf{Prune} et \textbf{Bagging}. L'arbre de décision reste inchangé avec \textbf{Prune}

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.height="50%", out.width="50%", fig.cap="L'arbre dans les deux cas", fig.align='center'}
if(!require("rpart")) {
  install.packages("rpart")
  library("rpart")
}

if(!require("rpart.plot")) {
  install.packages("rpart.plot")
  library("rpart.plot")
}

if(!require("randomForest")) {
  install.packages("randomForest")
  library("randomForest")
}

#fitDT<-rpart(y~.,data=data.train,method="anova")
#rpart.plot(fitDT, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)# Arbre initial

#model.tree <- rpart(y~., data=data.train, method="anova", control = rpart.control(xval = 10, minbucket = 2))
#i.min<-which.min(model.tree$cptable[,4])
#cp.opt1<-model.tree$cptable[i.min,1]

#pruned_tree <- prune(model.tree, cp=cp.opt1)
#rpart.plot(pruned_tree, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)# Arbre initial
knitr::include_graphics("./tree.png")  


```

Enfin, on a appliqué le modèle SVM en utilisant de différent noyaux. Les noyaux le plus performants qu'on a observé sont le noyau laplacien et le noyeau gaussien. Pour estimer la valeur de l'hyper-paramètre C, on a utilisé \textbf{Cross validation}

```{r echo=FALSE, fig.align='default', fig.cap="Le choix de l'hyper-paramètre pour Linear Kernel, Laplace Kernel, Gaussian Kernel", fig.show='hold', out.height="30%", out.width="30%", message=FALSE, warning=FALSE}
knitr::include_graphics(c("./cv.svmLin.bmp","./cv.svmLap.png", "./cv.svmGau.bmp"))  
```
  
## Modèle retenu
```{r echo=FALSE, fig.cap="MSE pour Modèle linéaire, KNN, Lasso, Ridge, ElasticNet, Natural Spline, GAM Arbre de décision, Arbre de décicion élagué, Random Forest, SVM Linear Kernel, SVM Laplace Kernel, SVM Gaussian Kernel", warning=FALSE, message=FALSE}

#finalData <- as.data.frame(cbind(err.lin.mse, CV.noscale, err.las.mse, err.rid.mse, err.ela.mse, err.splinesnaturel.mse,err.gam.mse, err.tree.mse, err.tree.mse.prune, err.randomForest.mse, err.svmlin.mse, err.svmlap.mse, err.svmgau.mse))
#colnames(finalData) <- c("Linear model", "KNN", "Lasso", "Ridge", "ElasticNet", "NaturalSpline", "GAM", "Decision Tree", "Decision Tree with pruning", "RandomForest", "SVM Linear", "SVM Laplace", "SVM Gaussian")

#boxplot(finalData, cex.axis=0.4, ylab = "MSE par Cross Validation")
knitr::include_graphics("./boxplot_final.png")  

```

Nous avons donc obtenu comme modèle le plus performant : modèle \textbf{SVM} avec noyau gaussien, avec C=100. C'est ce modèle que l'on va utiliser pour prédire les cinématiques du bra d'un robot.
